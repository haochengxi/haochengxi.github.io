# section information
section:
  name: Projects
  id: projects
  enable: true
  weight: 5
  showOnNavbar: true
  # Can optionally hide the title in sections
  # hideTitle: true

buttons:
- name: All
  filter: "all"
- name: Quantization
  filter: "quantization"
- name: Sparsity
  filter: "sparsity"
- name: Training
  filter: "training"
- name: Inference
  filter: "inference"

projects:
- name: "Sparse VideoGen 2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation"
  image: images/svg_2_overview.png
  role: Co-First Author
  timeline: "Feb 2025 - Jun 2025"
  repo: "https://github.com/svg-project/Sparse-VideoGen"
  url: "https://arxiv.org/abs/2505.18875"
  summary: We propose a training-free sparse attention framework that uses semantic-aware permutation - 
           clustering and reordering tokens via k-means based on semantic similarity - to achieve a new 
           pareto-frontier in speed-quality tradeoff.
  tags: ["sparsity", "inference"]

- name: "Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity"
  image: images/svg_overview.png
  role: Co-First Author
  timeline: "Sep 2024 - Feb 2025"
  repo: "https://github.com/svg-project/Sparse-VideoGen"
  url: "https://arxiv.org/abs/2502.01776"
  summary: We identify the spatial head and temporal head pattern in attention map and propose to 
           use sparse attention to accelerate.
           Achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo.
  tags: ["sparsity", "inference"]

- name: "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache"
  image: images/quantspec_overview.png
  role: Co-First Author
  timeline: "Sep 2024 - Feb 2025"
  repo: "#"
  url: "https://arxiv.org/abs/2502.10424"
  summary: We propose a self-speculative decoding framework, QuantSpec, to speedup long-context inference.
           QuantSpec maintains high acceptance rates (>90%) and reliably provides consistent 
           end-to-end speedups upto ∼ 2.5×.
  tags: ["quantization", "inference"]

- name: "COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training"
  image: images/coat8_overview.png
  role: First Author
  timeline: "Feb 2024 - Sep 2024"
  repo: https://github.com/NVlabs/COAT
  url: "https://arxiv.org/abs/2410.19313"
  summary: We propose Dynamic range expansion for FP8 optimizer, and propose FP8 precision flow for FP8 activations. 
           Achieve Lossless performance, end-to-End 1.54x memory reduction and 1.43x training speedup over BF16.
  tags: ["quantization", "training"]

- name: "NVILA: Efficient Frontier Visual Language Models"
  image: images/vila_overview.png
  role: Contributor
  timeline: "Mar 2024 - Nov 2024"
  repo: "https://github.com/NVlabs/VILA"
  url: "https://arxiv.org/abs/2412.04468"
  summary: We propose a new frontier of visual language models, NVILA, to achieve reduces training costs by 4.5X, 
           fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X.
  tags: ["quantization", "training", "inference"]

- name: "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference"
  image: images/sparge_overview.png
  role: Contributor
  timeline: "Sep 2024 - Feb 2025"
  repo: "https://github.com/thu-ml/SpargeAttn"
  summary: We propose SpargeAttn, a universal sparse and quantized attention for any model inference. 
           Our method uses a two-stage online filter to select the most important tokens.
  tags: ["sparsity", "inference"]

- name: "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"
  image: images/jetfire_overview.png
  role: First Author
  timeline: "Sep 2023 - Jan 2024"
  repo: https://github.com/thu-ml/Jetfire-INT8Training/tree/main
  url: "https://arxiv.org/abs/2403.12422"
  summary: Propose to INT8 precision flow and per-block quantization to enable INT8 pretraining of transformers.
           Demonstrate effectiveness on GPT2-774M model.
           Achieve End-to-End 1.42x training speedup and 1.49x memory reduction.
  tags: ["quantization", "training"]


- name: "Training Transformers with 4-bit Integers"
  image: images/int4train_overview.png
  role: First Author
  timeline: "Apr 2022 - Dec 2022"
  repo: https://github.com/haochengxi/Train_Transformers_with_INT4
  url: "https://arxiv.org/abs/2306.11987"
  summary: Propose Hadamard Quantizer and Leverage Score Sampling to enable INT4 Precision Matmul in training for speedup.
           Both the forward and backward pass are quantized into INT4 precision for maximized speedup.
           Outperforms all existing 4-bit training baselines.
  tags: ["quantization", "training"]
